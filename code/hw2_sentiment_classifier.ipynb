{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e449b3",
   "metadata": {},
   "source": [
    "# Homework 2: Sentiment Classifier\n",
    "\n",
    "The second programming assignment will familiarize you with the use of machine learning for text\n",
    "classification, and the use of prebuilt lexicons and bag of words to build custom features.\n",
    "\n",
    "The primary objective for the assignment is the same as the first assignment: to predict the sentiment of a movie review. We will be providing you with the dataset containing the text of the movie reviews from IMDB, and for each review, you have to predict whether the review is positive or negative.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is the same as the first homework. The filecontains a similar structure for the first homework. \n",
    "\n",
    "\n",
    "## Kaggle\n",
    "\n",
    "As with HW1, you can make at most three submissions each day, so we encourage you to test\n",
    "your submission files early and observe the performance of your system. By the end of the submission period, you will have to select the two submissions, one for default and one for custom (more on this later).\n",
    "\n",
    "\n",
    "## Source Code\n",
    "\n",
    "Some initial code contains methods for loading the data and lexicons, and calling the methods to run and evaluate your classifier. It also contains the code to output the submission file from your classifier (called ```rf_custom_text.csv```) that you will submit to Kaggle. Your directory structure should look like this.\n",
    "```\n",
    "hw2  \n",
    "│\n",
    "└───code\n",
    "│    └───hw2_sentiment_classifier.ipynb\n",
    "│    └───lexicon_reader.py\n",
    "└───data\n",
    "│    └───lexicon\n",
    "│        │   inqtabs.txt\n",
    "│        └───SentiWordNet_3.0.0_20130122.txt\n",
    "│    └───test\n",
    "│        │   0.txt\n",
    "│        │   1.txt\n",
    "│        │   ...   \n",
    "│        └───24999.txt\n",
    "│    └───train\n",
    "│        │   0.txt\n",
    "│        │   1.txt\n",
    "│        │   ...   \n",
    "│        └───24999.txt\n",
    "│    └───train.csv\n",
    "└───output\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad61a09",
   "metadata": {},
   "source": [
    "## What to submit?\n",
    "\n",
    "Prepare and submit a single write-up ( **PDF, maximum 3 pages** ) with Python source code (custom_features.py, error_analysis.ipynb, and ml_sentiment.py) compressed in a zip file to Canvas. **Do not include your student ID number** , since we might share it with the class if it’s worth highlighting. The write-up pdf and code zip file should be submitted separately on Canvas. The pdf should include:\n",
    "\n",
    "### Part 1. Preliminaries, 10 points\n",
    "Kaggle Team name and Kaggle accuracy of best default and custom models. The team with the best score in the competition has 10 points, the 2nd team has 9 points, the third has 8, and the others earn 7 points. You can make ***at most _three_*** submissions each day, so we encourage you to test your submission files early, and observe the performance of your system.\n",
    "\n",
    "- Start with a single line header: ```Id, Category```\n",
    "- For each of the unlabeled speech (sorted by name) there is a line containing an increasing integer index (i.e. line number 1), then a comma, and then the string label prediction of that speech.\n",
    "- See ```sample_sol.csv``` for example.\n",
    "\n",
    "### Part 2. Default Features, 20 points\n",
    "Tune classifiers on default features (include the selected range of parameters in the code), and submit your predictions to Kaggle. Include the accuracies of the best classifier for each (LR and RF) and the two plots in the write-up, and a few sentences on how you picked the range.\n",
    "\n",
    "### Part 3. Custom Features, 35 points\n",
    "Implement your custom features and vectorizers in the code, and train and tune the classifiers. Submit the predictions to Kaggle, identify the best classifier for each, and include the accuracy obtained in the report. Try at least five different sets of features and vectorizers with at least five other parameters. Include the description and comparison of your features and the vectorizers in a few sentences in the write-up.\n",
    "\n",
    "### Part 4. Analysis, 30 points\n",
    "Select the best classifier with default features and the best classifier with custom\n",
    "features. The analysis will focus on comparing these two classifiers. First, use eli5 to generate the global importance weights for both classifiers in the notebook (show_weights), and in a few sentences in the write-up, describe what is different between them. Then, in the notebook, generate 2 examples each where the classifiers disagree: (i) positive reviews, where only the default is correct, (ii) positive reviews, where only the custom is correct, (iii) negative reviews, where only the default is correct, and (iv) negative reviews, where only the custom is correct. Also, include 2 examples each of when both classifiers are correct, and both are incorrect. In the write-up, include a paragraph describing the insights these errors provide about the differences between the classifiers, especially the advantages/disadvantages of your custom features.\n",
    "\n",
    "### Part 5. Statement of Collaborations, 5 points\n",
    "\n",
    "It is mandatory to include a Statement of Collaboration in each submission with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones) and what was discussed. All students are required to follow the academic honesty guidelines posted on the course website. For programming assignments, in particular, we encourage the students to organize (perhaps using Piazza) to discuss the task descriptions, requirements, bugs in our code, and the relevant technical content before they start working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not allowed to take anything written or drawn away from these discussions (i.e., no photographs of the blackboard, written notes, referring to Piazza, etc.). Especially after you have started working on the assignment, try to restrict the discussion to Piazza as much as possible, so that there is no doubt as to the extent of your collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import eli5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "\n",
    "import lexicon_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d7fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, start_idx, end_idx):\n",
    "        self.data = data\n",
    "        self.reviews = [row['Review'] for row in data[start_idx:end_idx]]\n",
    "        self.labels = [row['Category'] for row in data[start_idx:end_idx]]\n",
    "        self.vecs = None\n",
    "\n",
    "def get_training_and_dev_data(filedir, dev_rate=0.2):\n",
    "    with open(os.path.join(filedir, 'train.csv'), 'r', encoding='utf-8') as csvfile:\n",
    "        data = [row for row in csv.DictReader(csvfile, delimiter=',')]\n",
    "        for entry in data:\n",
    "            with open(os.path.join(filedir, 'train', entry['FileIndex'] + '.txt'), 'r', encoding='utf-8') as reviewfile:\n",
    "                entry['Review'] = reviewfile.read()\n",
    "    dev_idx = int(len(data) * (1 - dev_rate))\n",
    "    return Dataset(data, 0, dev_idx), Dataset(data, dev_idx, len(data))\n",
    "\n",
    "def get_test_data(filedir, output_file_name):\n",
    "    testfiledir = os.path.join(filedir, 'test')\n",
    "    with open(output_file_name, 'w', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, delimiter=',', fieldnames=['FileIndex', 'Category'])\n",
    "        writer.writeheader()\n",
    "        for filename in sorted(os.listdir(testfiledir), key=lambda x: int(os.path.splitext(x)[0])):\n",
    "            with open(os.path.join(testfiledir, filename), 'r', encoding='utf-8') as reviewfile:\n",
    "                fileindex = os.path.splitext(filename)[0]\n",
    "                review = reviewfile.read()\n",
    "                yield (fileindex, review)\n",
    "\n",
    "def write_predictions(filedir, classifier, output_file_name):\n",
    "    with open(output_file_name, 'w', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, delimiter=',', fieldnames=['FileIndex', 'Category'])\n",
    "        writer.writeheader()\n",
    "        for (fileindex, review) in get_test_data(filedir, output_file_name):\n",
    "            prediction = dict()\n",
    "            prediction['Id'] = fileindex\n",
    "            prediction['Category'] = classifier.predict([review])[0]\n",
    "            writer.writerow(prediction)\n",
    "\n",
    "def get_trained_classifier(data, model, features):\n",
    "    ppl = make_pipeline(features, model)\n",
    "    return ppl.fit(data.reviews, data.labels)\n",
    "\n",
    "def get_custom_features(filedir):\n",
    "    return FeatureUnion([\n",
    "        ('custom_feats', make_pipeline(CustomFeats(filedir), DictVectorizer())),\n",
    "        ('bag_of_words', get_custom_vectorizer())\n",
    "    ])\n",
    "\n",
    "def save(classifier, filedir, output_file_path):\n",
    "    with open(output_file_path + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    write_predictions(filedir, classifier, output_file_path + \"_test.csv\")\n",
    "\n",
    "def load_classifier(input_file_path):\n",
    "    return pickle.load(open(input_file_path, 'rb'))\n",
    "\n",
    "def plot(xs, train_accuracy_list, dev_accuracy_list, output_file_path=None):\n",
    "    plt.clf()\n",
    "    plt.plot(xs, train_accuracy_list, label='train')\n",
    "    plt.plot(xs, dev_accuracy_list, label='dev')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    if output_file_path is not None:\n",
    "        plt.savefig(output_file_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b29ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "filedir = '../data'\n",
    "print(\"Reading data\")\n",
    "train_data, dev_data = get_training_and_dev_data(filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629933",
   "metadata": {},
   "source": [
    "## Part 2. Default Features\n",
    "We are providing code for training a machine learning classifier for sentiment classification using unigrams as features, i.e. ```CountVectorizer()```. The first goal is to optimize the hyper-parameters of logistic regression by modifying ```get_tuned_lr```. The regularization weight C is the primary hyper-parameter for logistic regression. Currently, the range for the parameter is ```np.arange(0.5, 3.5, 0.5)``` but this should be modified. When running this function, you will see both training and dev accuracy printed. There will also be a plot ```lr.png``` that will be saved that you can view. Based on these, adjust the range for C. \n",
    "\n",
    "The next goal is to optimize the parameters for random forest. To do so, you need to modify ```get_tuned_rf```. ```n_estimators``` is the parameter of interest used by random forest. Currently, the range is set to ```np.arange(5, 35, 5)``` but this should also be modified. Like before, when running this function, you will see both training accuracy and dev accuracy, and the plot ```rf.png``` will be saved. Based on what you see, adjust the parameters accordingly.\n",
    "\n",
    "Running ```save(tuned_lr, filedir, ‘lr_default’)``` will save the classifier as ```lr_default.pkl``` which you will need for your error analysis. It will also run the classifier on the test set and save the results as ```lr_default_test.csv```, which you can upload to Kaggle. Similarly, the next line will output the files ```rf_default.pkl``` and ```rf_default_test.csv```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c282f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_lr(train, dev, features, output_file_path='./lr.png'):\n",
    "    train_vecs = features.fit_transform(train.reviews)\n",
    "    dev_vecs = features.transform(dev.reviews)\n",
    "    train_accuracy_list = list()\n",
    "    dev_accuracy_list = list()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # TODO: You will change this range, or may want to use np.logspace instead of np.arrange\n",
    "    cs = np.arange(0.5, 3.5, 0.5)  \n",
    "    \n",
    "    for c in cs:\n",
    "        model = LogisticRegression(C=c)\n",
    "        model.fit(train_vecs, train.labels)\n",
    "        train_preds = model.predict(train_vecs)\n",
    "        dev_preds = model.predict(dev_vecs)\n",
    "        (train_score, dev_score) = (accuracy_score(train.labels, train_preds), accuracy_score(dev.labels, dev_preds))\n",
    "        print(\"Train Accuracy:\", train_score, \", Dev Accuracy:\", dev_score)\n",
    "        train_accuracy_list.append(train_score)\n",
    "        dev_accuracy_list.append(dev_score)\n",
    "    plot(cs, train_accuracy_list, dev_accuracy_list, output_file_path)\n",
    "    best_model = LogisticRegression(C=cs[np.argmax(dev_accuracy_list)])\n",
    "    return get_trained_classifier(train, best_model, features)\n",
    "\n",
    "\n",
    "def get_tuned_rf(train, dev, features, output_file_path='./rf.png'):\n",
    "    train_vecs = features.fit_transform(train.reviews)\n",
    "    dev_vecs = features.transform(dev.reviews)\n",
    "    train_accuracy_list = list()\n",
    "    dev_accuracy_list = list()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # TODO: You will change this range, and try different parameters to tune RF model\n",
    "    n_estimators = np.arange(5, 35, 5)  \n",
    "    \n",
    "    for num_estimator in n_estimators:\n",
    "        model = RandomForestClassifier(n_estimators=num_estimator)\n",
    "        model.fit(train_vecs, train.labels)\n",
    "        train_preds = model.predict(train_vecs)\n",
    "        dev_preds = model.predict(dev_vecs)\n",
    "        (train_score, dev_score) = (accuracy_score(train.labels, train_preds), accuracy_score(dev.labels, dev_preds))\n",
    "        print(\"Train Accuracy:\", train_score, \", Dev Accuracy:\", dev_score)\n",
    "        train_accuracy_list.append(train_score)\n",
    "        dev_accuracy_list.append(dev_score)\n",
    "    plot(n_estimators, train_accuracy_list, dev_accuracy_list, output_file_path)\n",
    "    best_model = RandomForestClassifier(n_estimators=n_estimators[np.argmax(dev_accuracy_list)])\n",
    "    return get_trained_classifier(train, best_model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9a1e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some example code to get a trained classifier\n",
    "print(\"Training model\")\n",
    "lr_with_default = get_trained_classifier(train_data, LogisticRegression(), CountVectorizer())\n",
    "rf_with_default = get_trained_classifier(train_data, RandomForestClassifier(), CountVectorizer())\n",
    "\n",
    "# You can see some of the predictions of the classifier by running the following code\n",
    "print(lr_with_default.predict([\"This movie sucks!\", \"This movie is great!\"]))\n",
    "print(rf_with_default.predict([\"This movie sucks!\", \"This movie is great!\"]))\n",
    "\n",
    "# You can then experiment with tuning the classifiers\n",
    "# Experiment with the parameters in the get_tuned_lr and get_tuned_rf methods\n",
    "# Look at the files lr.png and rf.png that are saved after running each of these functions below\n",
    "print(\"Tuning model\")\n",
    "tuned_lr = get_tuned_lr(train_data, dev_data, CountVectorizer())\n",
    "tuned_rf = get_tuned_rf(train_data, dev_data, CountVectorizer())\n",
    "\n",
    "# After playing with the parameters and finding a good classifier, you can save\n",
    "# This will save the classifier to a pickle object which you can then load later from when doing your error analysis\n",
    "# As well as this will run the classifier on the test set which you can then upload to kaggle\n",
    "print(\"Saving model and predictions\")\n",
    "save(tuned_lr, filedir, 'lr_default')\n",
    "save(tuned_rf, filedir, 'rf_default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c64df",
   "metadata": {},
   "source": [
    "## Part 3. Custom Features and Vectorizers\n",
    "The next goal for the assignment is to improve upon these classifiers by introducing your own features. Similar to the in-class activity in week 4, you will design features that utilize lexicons and regular expressions, etc., and ```experiment with the vectorizer``` (e.g., unigrams vs. bigrams, counts vs. TF-IDF, etc.). To implement\n",
    "these ```features```, you will need to modify the features function, and you can change\n",
    "the ```vectorizer```.\n",
    "\n",
    "As before, tune the parameters for both logistic regression and random forest, but this time with your custom features. \n",
    "\n",
    "Run the save methods to save the classifiers (```lr_custom.pkl``` and ```rf_custom.pkl```) and predictions (```lr_custom_text.csv``` and ```rf_custom_text.csv```), and upload the latter files to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, filedir):\n",
    "        self.feat_names = set()\n",
    "        lexicon_dir = os.path.join(filedir, 'lexicon')\n",
    "        self.inqtabs_dict = lexicon_reader.read_inqtabs(os.path.join(lexicon_dir, 'inqtabs.txt'))\n",
    "        self.swn_dict = lexicon_reader.read_senti_word_net(os.path.join(lexicon_dir, 'SentiWordNet_3.0.0_20130122.txt'))\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def word_count(review):\n",
    "        words = review.split(' ')\n",
    "        return len(words)\n",
    "\n",
    "    def pos_count(self, review):\n",
    "        words = review.split(' ')\n",
    "        count = 0\n",
    "        for word in words:\n",
    "            if word in self.inqtabs_dict.keys() and self.inqtabs_dict[word] == lexicon_reader.POS_LABEL:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def features(self, review):\n",
    "        return {\n",
    "            # -------------------------------------------------------------------------\n",
    "            # 4 example features \n",
    "            # TODO: Add your own here e.g. word_count, and pos_count\n",
    "            'length': len(review),\n",
    "            'num_sentences': review.count('.'),\n",
    "            'num_words': self.word_count(review),\n",
    "            'pos_count': self.pos_count(review)  \n",
    "        }\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return list(self.feat_names)\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        feats = []\n",
    "        for review in reviews:\n",
    "            f = self.features(review)\n",
    "            [self.feat_names.add(k) for k in f]\n",
    "            feats.append(f)\n",
    "        return feats\n",
    "\n",
    "\n",
    "def get_custom_vectorizer():\n",
    "    # -------------------------------------------------------------------------\n",
    "    #TODO: Experiment with different vectorizers\n",
    "    return CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ae4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different features by modifiying custom_features.py and test your accuracy by running:\n",
    "# (Again, you can look at the lr.png  and rf.png that are saved after running each of these functions)\n",
    "print(\"Tuning model\")\n",
    "tuned_lr = get_tuned_lr(train_data, dev_data, get_custom_features(filedir))\n",
    "tuned_rf = get_tuned_rf(train_data, dev_data, get_custom_features(filedir))\n",
    "\n",
    "print(\"Saving model and predictions\")\n",
    "save(tuned_lr, filedir, 'lr_custom')\n",
    "save(tuned_rf, filedir, 'rf_custom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119a24f",
   "metadata": {},
   "source": [
    "## Part 4. Error Analysis\n",
    "Along with tuning classifiers and designing features to achieve as high of accuracy as possible, you also need to perform an analysis of the classifier in this assignment. For this analysis, modify the function to get metrics and feature weights for all four of your best classifiers (note, it is currently set up to only compute metrics for two, so you will need to either modify it to get metrics for all four or run it twice). You may need to update ```load_classifier(’lr_default.pkl’)``` replacing ```lr_default.pkl``` with the model you have (the one that was saved when you run the saved method earlier).\n",
    "\n",
    "The rest of the notebook also contains the two primary approaches for analysis that use the ```eli5``` package: (1) global importance weights of individual classifiers, and (2) weights of individual words, for example, predictions. The notebook also includes code for easily comparing classifiers to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b066421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "def get_error_type(pred, label):\n",
    "    # return the type of error: tp,fp,tn,fn\n",
    "    if pred == label:\n",
    "        return \"tp\" if pred == '1' else \"tn\"\n",
    "    return \"fp\" if pred == '1' else \"fn\"\n",
    "\n",
    "# Change this for your different classifiers\n",
    "classifier1 = load_classifier('lr_default.pkl')\n",
    "classifier2 = load_classifier('rf_custom.pkl')\n",
    "\n",
    "# Create pandas dataframe\n",
    "predictions = pd.DataFrame.from_dict(dev_data.data)\n",
    "\n",
    "# Classify data points using classifier1\n",
    "predictions['Classifier1Prediction'] = classifier1.predict(predictions['Review'])\n",
    "predictions['Classifier1ErrorType'] = predictions.apply(lambda row: get_error_type(row['Classifier1Prediction'], row['Category']), axis=1)\n",
    "\n",
    "# Classify data points using classifier 2\n",
    "predictions['Classifier2Prediction'] = classifier2.predict(predictions['Review'])\n",
    "predictions['Classifier2ErrorType'] = predictions.apply(lambda row: get_error_type(row['Classifier2Prediction'], row['Category']), axis=1)\n",
    "\n",
    "# Get metrics for each classifier\n",
    "def print_metrics(error_type_counts):\n",
    "    accuracy = (error_type_counts['tp'] + error_type_counts['tn']) / sum(error_type_counts)\n",
    "    precision = error_type_counts['tp'] / (error_type_counts['tp'] + error_type_counts['fp'])\n",
    "    recall = error_type_counts['tp'] / (error_type_counts['tp'] + error_type_counts['fn'])\n",
    "    print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \"\\nF1:\", 2 * precision*recall/(precision + recall))\n",
    "\n",
    "print(\"Classifier1 Metrics\")\n",
    "print_metrics(predictions['Classifier1ErrorType'].value_counts())\n",
    "print(\"\\nClassifier2 Metrics\")\n",
    "print_metrics(predictions['Classifier2ErrorType'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(classifier1, top=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(classifier2, top=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f5d78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See some examples of errors for each classifier\n",
    "# Modify the code to get false negatives and errors for Classifier2)\n",
    "predictions[predictions['Classifier1ErrorType'] == 'fp'].sample(10)\n",
    "\n",
    "# See where they disagree\n",
    "# Modify the code to find cases where one classifier's prediction is correct but the other is incorrect\n",
    "predictions['ClassifiersAgree'] = predictions['Classifier1Prediction'] == predictions['Classifier2Prediction']\n",
    "disagreements = predictions[predictions['ClassifiersAgree'] == False]\n",
    "print(\"# Cases where the two classifiers disagree:\", len(disagreements), \"->\", len(disagreements) / len(predictions) * 100, \"%\")\n",
    "disagreements.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733d926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
